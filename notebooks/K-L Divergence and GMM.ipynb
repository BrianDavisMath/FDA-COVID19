{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler divergence\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">UPDATE: GMM and K-L Divergence did not work. GMM only returned binary values from predict_proba.</span>\n",
    "\n",
    "\n",
    "Use K-L divergence to determine the effect of target variable (activity) values (0 or 1) on a Gaussian mixture model over the data.\n",
    "\n",
    "Apply this only to the continuous features, separately for cid and pid.\n",
    "\n",
    "References:\n",
    "\n",
    "* [KL Divergence Python Example](https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810)—a nice explanation and example\n",
    "* [Stackexchange: Calculating KL Divergence in Python\n",
    "](https://datascience.stackexchange.com/questions/9262/calculating-kl-divergence-in-python)—some good notes on which Python libraries to use\n",
    "* [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)—seems relevant here\n",
    "* [Sensitivity Analysis in Gaussian Bayesian Networks Using a Divergence Measure](https://www.researchgate.net/publication/233216409_Sensitivity_Analysis_in_Gaussian_Bayesian_Networks_Using_a_Divergence_Measure)\n",
    "* [Clustering with Gaussian Mixture Models](https://pythonmachinelearning.pro/clustering-with-gaussian-mixture-models/)\n",
    "* [scikit-learn Gaussian mixture models](https://scikit-learn.org/stable/modules/mixture.html)\n",
    "* [Approximating the Kullback Leibler Divergence Between Gaussian Mixture Models](https://www.researchgate.net/publication/4249249_Approximating_the_Kullback_Leibler_Divergence_Between_Gaussian_Mixture_Models)\n",
    "* [Stackoverlflow: KL-Divergence of two GMMs\n",
    "](https://stackoverflow.com/questions/26079881/kl-divergence-of-two-gmms)\n",
    "* [Stackexchange: Trying to implement the Jensen-Shannon Divergence for Multivariate Gaussians](https://stats.stackexchange.com/questions/345915/trying-to-implement-the-jensen-shannon-divergence-for-multivariate-gaussians)—related and refering to the above Stackoverflow Q&A\n",
    "* [Stackoverlflow: predict_proba is not working for my gaussian mixture model (sklearn, python)\n",
    "](https://stackoverflow.com/questions/56993070/predict-proba-is-not-working-for-my-gaussian-mixture-model-sklearn-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the features files for CIDs and PIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = '../data/FDA-COVID19_files_v1.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the individual feature sets as data frames\n",
    "  def __load_feature_files():\n",
    "    print('===============================================')\n",
    "    print('dragon_features.csv')\n",
    "    print('===============================================')\n",
    "    # note need to set the data_type to object because it complains, otherwise that the types vary.\n",
    "    df_dragon_features = __load_data(data_loc+'drug_features/dragon_features.csv', data_type=object)\n",
    "    \n",
    "    # rename the dragon features since there are duplicate column names in the protein binding-sites data.\n",
    "    df_dragon_features.columns = ['cid_'+col for col in df_dragon_features.columns]\n",
    "    \n",
    "    # handle na values in dragon_features\n",
    "    # Many cells contain \"na\" values. Find the columns that contain 2% or \n",
    "    # less of these values and retain them, throwing away the rest. \n",
    "    # Then mean-impute the \"na\" values in the remaining columns.\n",
    "    pct_threshold = 2\n",
    "    na_threshold = int(len(df_dragon_features)*pct_threshold/100)\n",
    "    ok_cols = []\n",
    "\n",
    "    for col in df_dragon_features:\n",
    "        na_count = df_dragon_features[col].value_counts().get('na')\n",
    "        if (na_count or 0) <= na_threshold:\n",
    "            ok_cols.append(col)\n",
    "\n",
    "    print('number of columns where the frequency of \"na\" values is <= {}%: {}.'.format(pct_threshold, len(ok_cols)))\n",
    "    \n",
    "    df_dragon_features = df_dragon_features[ok_cols]\n",
    "\n",
    "    # convert all values except \"na\"s to numbers and set \"na\" values to NaNs.\n",
    "    df_dragon_features = df_dragon_features.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    columns_missing_values = df_dragon_features.columns[df_dragon_features.isnull().any()].tolist()\n",
    "    print('{} columns with missing values.'.format(len(columns_missing_values)))\n",
    "\n",
    "    # replace NaNs with column means\n",
    "    df_dragon_features.fillna(df_dragon_features.mean(), inplace=True)\n",
    "\n",
    "    columns_missing_values = df_dragon_features.columns[df_dragon_features.isnull().any()].tolist()\n",
    "    print('{} columns with missing values (after imputing): {}'.format(len(columns_missing_values), \n",
    "                                                                       columns_missing_values))    \n",
    "    print('===============================================')\n",
    "    print('bs_features_reduced.csv')\n",
    "    print('===============================================')\n",
    "    df_binding_sites = __load_data(data_loc+'protein_features/bs_features_reduced.csv')\n",
    "    \n",
    "    # Name the index to 'pid' to allow joining to other feaure files later.\n",
    "    df_binding_sites.index.name = 'pid'\n",
    "    \n",
    "    print('===============================================')\n",
    "    print('expasy.csv')\n",
    "    print('===============================================')\n",
    "    df_expasy = __load_data(data_loc+'protein_features/expasy.csv')\n",
    "    \n",
    "    print('===============================================')\n",
    "    print('profeat.csv')\n",
    "    print('===============================================')\n",
    "    df_profeat = __load_data(data_loc+'protein_features/profeat.csv')\n",
    "    \n",
    "    # Name the index to 'pid' to allow joining to other feaure files later.\n",
    "    df_profeat.index.name = 'pid'\n",
    "    \n",
    "    # profeat has some missing values.\n",
    "    s = df_profeat.isnull().sum(axis = 0)\n",
    "\n",
    "    print('number of missing values for each column containing them is: {}'.format(len(s[s > 0])))\n",
    "\n",
    "    # Drop the rows that have missing values.\n",
    "    df_profeat.dropna(inplace=True)\n",
    "    print('number of rows remaining, without NaNs: {:,}'.format(len(df_profeat)))\n",
    "    \n",
    "    return {'df_dragon_features': df_dragon_features,\n",
    "           'df_binding_sites': df_binding_sites,\n",
    "           'df_expasy': df_expasy,\n",
    "           'df_profeat': df_profeat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a specific features CSV file\n",
    "  def __load_data(path, data_type=None):\n",
    "    if data_type:\n",
    "        df = pd.read_csv(path, index_col=0, dtype=data_type)\n",
    "    else:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "    print('Number of rows: {:,}'.format(len(df)))\n",
    "    print('Number of columns: {:,}'.format(len(df.columns)))\n",
    "    \n",
    "    columns_missing_values = df.columns[df.isnull().any()].tolist()\n",
    "    print('{} columns with missing values'.format(len(columns_missing_values)))\n",
    "    \n",
    "    print(df.head(2))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "dragon_features.csv\n",
      "===============================================\n",
      "Number of rows: 88,105\n",
      "Number of columns: 3,839\n",
      "0 columns with missing values\n",
      "              MW                AMW      Sv                  Se  \\\n",
      "cid                                                               \n",
      "72792562  474.67  6.781000000000001  41.039              70.101   \n",
      "44394609  546.48              8.674  43.185  63.538000000000004   \n",
      "\n",
      "                          Sp                 Si     Mv                  Me  \\\n",
      "cid                                                                          \n",
      "72792562   43.54600000000001  80.52199999999999  0.586               1.001   \n",
      "44394609  45.233000000000004             69.993  0.685  1.0090000000000001   \n",
      "\n",
      "             Mp     Mi  ... Psychotic-80 Psychotic-50 Hypertens-80  \\\n",
      "cid                     ...                                          \n",
      "72792562  0.622   1.15  ...            0            0            0   \n",
      "44394609  0.718  1.111  ...            0            0            0   \n",
      "\n",
      "         Hypertens-50 Hypnotic-80 Hypnotic-50 Neoplastic-80 Neoplastic-50  \\\n",
      "cid                                                                         \n",
      "72792562            0           0           0             0             0   \n",
      "44394609            0           0           0             0             0   \n",
      "\n",
      "         Infective-80 Infective-50  \n",
      "cid                                 \n",
      "72792562            0            0  \n",
      "44394609            0            0  \n",
      "\n",
      "[2 rows x 3839 columns]\n",
      "number of columns where the frequency of \"na\" values is <= 2%: 3640.\n",
      "3565 columns with missing values.\n",
      "0 columns with missing values (after imputing): []\n",
      "===============================================\n",
      "bs_features_reduced.csv\n",
      "===============================================\n",
      "Number of rows: 4,209\n",
      "Number of columns: 5,258\n",
      "0 columns with missing values\n",
      "          TAA    MAA    AAA    SAA    RAA    VAA    LAA    KAA    EAA    QAA  \\\n",
      "Q9BZP6   True  False  False  False  False  False  False  False  False  False   \n",
      "P43003  False  False  False  False  False  False  False  False  False  False   \n",
      "\n",
      "        ...  YYY_VYY  CYY_DYY  AYY_EYY  IYY_FYY  KYY_LYY  HYY_NYY  GYY_PYY  \\\n",
      "Q9BZP6  ...    False    False    False    False    False    False    False   \n",
      "P43003  ...    False    False    False    False    False    False    False   \n",
      "\n",
      "        TYY_RYY  QYY_WYY    AZZ  \n",
      "Q9BZP6    False    False  False  \n",
      "P43003    False    False  False  \n",
      "\n",
      "[2 rows x 5258 columns]\n",
      "===============================================\n",
      "expasy.csv\n",
      "===============================================\n",
      "Number of rows: 4,201\n",
      "Number of columns: 7\n",
      "0 columns with missing values\n",
      "        helical   beta   coil  veryBuried  veryExposed  someBuried  \\\n",
      "pid                                                                  \n",
      "10GS_A    0.536  0.096  0.368       0.292        0.254       0.234   \n",
      "1A2C_H    0.089  0.378  0.533       0.313        0.301       0.212   \n",
      "\n",
      "        someExposed  \n",
      "pid                  \n",
      "10GS_A        0.220  \n",
      "1A2C_H        0.174  \n",
      "===============================================\n",
      "profeat.csv\n",
      "===============================================\n",
      "Number of rows: 4,167\n",
      "Number of columns: 849\n",
      "80 columns with missing values\n",
      "        [G1.1.1.1]  [G1.1.1.2]  [G1.1.1.3]  [G1.1.1.4]  [G1.1.1.5]  \\\n",
      "10GS_A    7.177033    1.913876    6.220096    4.784689    3.349282   \n",
      "1A2C_H    4.633205    2.702703    6.177606    5.791506    3.474903   \n",
      "\n",
      "        [G1.1.1.6]  [G1.1.1.7]  [G1.1.1.8]  [G1.1.1.9]  [G1.1.1.10]  ...  \\\n",
      "10GS_A    8.612440    0.956938    3.349282    5.741627    15.311005  ...   \n",
      "1A2C_H    8.494208    1.930502    6.177606    7.335907     7.722008  ...   \n",
      "\n",
      "        [G7.1.1.71]  [G7.1.1.72]  [G7.1.1.73]  [G7.1.1.74]  [G7.1.1.75]  \\\n",
      "10GS_A    -0.001570     0.002455     0.001529     0.007086     0.001563   \n",
      "1A2C_H    -0.003062    -0.005141     0.003201     0.003635    -0.004960   \n",
      "\n",
      "        [G7.1.1.76]  [G7.1.1.77]  [G7.1.1.78]  [G7.1.1.79]  [G7.1.1.80]  \n",
      "10GS_A     0.001808    -0.003158    -0.003736     0.005282     0.004062  \n",
      "1A2C_H    -0.005522     0.005161     0.001182     0.007673     0.003012  \n",
      "\n",
      "[2 rows x 849 columns]\n",
      "number of missing values for each column containing them is: 80\n",
      "number of rows remaining, without NaNs: 4,161\n"
     ]
    }
   ],
   "source": [
    "feature_sets = __load_feature_files()\n",
    "\n",
    "df_dragon_features = feature_sets['df_dragon_features']\n",
    "df_binding_sites = feature_sets['df_binding_sites']\n",
    "df_expasy = feature_sets['df_expasy']\n",
    "df_profeat = feature_sets['df_profeat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set: Merge the features for CIDs and PIDs with the interactions.\n",
    "\n",
    "This yields a set of CID features and a set of PID features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 7,972\n",
      "Number of columns: 5\n",
      "0 columns with missing values\n",
      "    cid       pid  activity  cid_binary_weights  pid_binary_weights\n",
      "0   938  AAB59829         1            0.952894            0.996703\n",
      "1  1986  AAB59829         1            0.975912            0.996703\n"
     ]
    }
   ],
   "source": [
    "validation_interactions = '../data/v5/validation_interactions_v5.csv'\n",
    "df_interactions = __load_data(validation_interactions)\n",
    "\n",
    "df_pid_vld = pd.merge(df_interactions, df_expasy, on='pid', how='inner')\n",
    "df_pid_vld = pd.merge(df_pid_vld, df_profeat, on='pid', how='inner')\n",
    "\n",
    "df_cid_vld = pd.merge(df_interactions, df_dragon_features, on='cid', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 897\n",
      "Number of columns: 3\n",
      "0 columns with missing values\n",
      "          cid     pid  activity\n",
      "98   13069167  Q01668         0\n",
      "472  90430761  P53634         0\n"
     ]
    }
   ],
   "source": [
    "test_interactions = '../data/v4/test_interactions_v4.csv'\n",
    "df_test_interactions = __load_data(test_interactions)\n",
    "\n",
    "df_pid_test = pd.merge(df_test_interactions, df_expasy, on='pid', how='inner')\n",
    "df_pid_test = pd.merge(df_pid_test, df_profeat, on='pid', how='inner')\n",
    "\n",
    "df_cid_test = pd.merge(df_test_interactions, df_dragon_features, on='cid', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 88)\n",
      "(29, 849)\n",
      "(28, 938)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>length</th>\n",
       "      <th>weight</th>\n",
       "      <th>pI</th>\n",
       "      <th>A Total</th>\n",
       "      <th>A Percent</th>\n",
       "      <th>R Total</th>\n",
       "      <th>R Percent</th>\n",
       "      <th>N Total</th>\n",
       "      <th>N Percent</th>\n",
       "      <th>...</th>\n",
       "      <th>[G7.1.1.71]</th>\n",
       "      <th>[G7.1.1.72]</th>\n",
       "      <th>[G7.1.1.73]</th>\n",
       "      <th>[G7.1.1.74]</th>\n",
       "      <th>[G7.1.1.75]</th>\n",
       "      <th>[G7.1.1.76]</th>\n",
       "      <th>[G7.1.1.77]</th>\n",
       "      <th>[G7.1.1.78]</th>\n",
       "      <th>[G7.1.1.79]</th>\n",
       "      <th>[G7.1.1.80]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QHD43415</td>\n",
       "      <td>7096</td>\n",
       "      <td>794057.79</td>\n",
       "      <td>6.32</td>\n",
       "      <td>487</td>\n",
       "      <td>6.9</td>\n",
       "      <td>244</td>\n",
       "      <td>3.4</td>\n",
       "      <td>384</td>\n",
       "      <td>5.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QHD43416</td>\n",
       "      <td>1273</td>\n",
       "      <td>141178.47</td>\n",
       "      <td>6.24</td>\n",
       "      <td>79</td>\n",
       "      <td>6.2</td>\n",
       "      <td>42</td>\n",
       "      <td>3.3</td>\n",
       "      <td>88</td>\n",
       "      <td>6.9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000696</td>\n",
       "      <td>-0.000724</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>-0.000548</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>-0.004617</td>\n",
       "      <td>-0.003577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QHD43417</td>\n",
       "      <td>275</td>\n",
       "      <td>31122.94</td>\n",
       "      <td>5.55</td>\n",
       "      <td>13</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>-0.000293</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.004706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QHD43418</td>\n",
       "      <td>75</td>\n",
       "      <td>8365.04</td>\n",
       "      <td>8.57</td>\n",
       "      <td>4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-0.001037</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.004847</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QHD43419</td>\n",
       "      <td>222</td>\n",
       "      <td>25146.62</td>\n",
       "      <td>9.51</td>\n",
       "      <td>19</td>\n",
       "      <td>8.6</td>\n",
       "      <td>14</td>\n",
       "      <td>6.3</td>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.005608</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>0.009114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 938 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pid  length     weight    pI  A Total  A Percent  R Total  R Percent  \\\n",
       "0  QHD43415    7096  794057.79  6.32      487        6.9      244        3.4   \n",
       "1  QHD43416    1273  141178.47  6.24       79        6.2       42        3.3   \n",
       "2  QHD43417     275   31122.94  5.55       13        4.7        6        2.2   \n",
       "3  QHD43418      75    8365.04  8.57        4        5.3        3        4.0   \n",
       "4  QHD43419     222   25146.62  9.51       19        8.6       14        6.3   \n",
       "\n",
       "   N Total  N Percent  ...  [G7.1.1.71]  [G7.1.1.72]  [G7.1.1.73]  \\\n",
       "0      384        5.4  ...     0.000527     0.000844     0.001346   \n",
       "1       88        6.9  ...    -0.000696    -0.000724     0.002304   \n",
       "2        8        2.9  ...     0.001989     0.001076    -0.000293   \n",
       "3        5        6.7  ...     0.000490     0.004200    -0.001037   \n",
       "4       11        5.0  ...     0.004468     0.003642     0.004706   \n",
       "\n",
       "   [G7.1.1.74]  [G7.1.1.75]  [G7.1.1.76]  [G7.1.1.77]  [G7.1.1.78]  \\\n",
       "0     0.000922     0.000320     0.000632     0.001052     0.001342   \n",
       "1     0.003106     0.000724     0.002734    -0.000548     0.000223   \n",
       "2     0.001269     0.004673     0.005930    -0.002706     0.000263   \n",
       "3     0.002456     0.004847     0.005366     0.002756     0.006903   \n",
       "4     0.005608     0.004701     0.005750     0.004577     0.006347   \n",
       "\n",
       "   [G7.1.1.79]  [G7.1.1.80]  \n",
       "0     0.000306     0.000374  \n",
       "1    -0.004617    -0.003577  \n",
       "2     0.003124     0.004706  \n",
       "3     0.001546     0.000493  \n",
       "4     0.005715     0.009114  \n",
       "\n",
       "[5 rows x 938 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: make sure that COVID pid features are non-negative when scaled.\n",
    "covid_path = data_loc+'coronavirus_features/'\n",
    "covid_expasy = covid_path+'coronavirus_expasy.csv'\n",
    "covid_profeat = covid_path+'coronavirus_profeat.csv'\n",
    "\n",
    "df_covid_expasy = pd.read_csv(covid_expasy, index_col=0)\n",
    "df_covid_profeat = pd.read_csv(covid_profeat, index_col=0)\n",
    "\n",
    "print(df_covid_expasy.shape)\n",
    "print(df_covid_profeat.shape)\n",
    "\n",
    "# pid index to column\n",
    "df_covid_expasy.reset_index(level=0, inplace=True)\n",
    "\n",
    "# sameish\n",
    "df_covid_profeat['pid'] = df_covid_profeat.index\n",
    "df_covid_profeat.head()\n",
    "\n",
    "df_covid = pd.merge(df_covid_expasy, df_covid_profeat,on='pid', how='inner')\n",
    "\n",
    "# Drop features that don't exist in the test set.\n",
    "desired_cols = [c for c in df_pid_test.columns.tolist() if c not in ['cid', 'activity']]\n",
    "df_covid = df_covid[desired_cols].copy()\n",
    "\n",
    "print(df_covid.shape)\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set: Merge the features for CIDs and PIDs with the interactions.\n",
    "\n",
    "This yields a set of CID features and a set of PID features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "optional_training_interactions_csv = '../data/v5/optional_training_interactions_v5.csv'\n",
    "required_training_interactions_csv = '../data/v5/required_training_interactions_v5.csv'\n",
    "\n",
    "optional_training_interactions = pd.read_csv(optional_training_interactions_csv, index_col=0)\n",
    "required_training_interactions = pd.read_csv(required_training_interactions_csv, index_col=0)\n",
    "\n",
    "drop_cols = ['cid_binary_weights', 'pid_binary_weights', 'activity_score']\n",
    "\n",
    "opt_unique = optional_training_interactions.drop(drop_cols, axis=1)\n",
    "req_unique = required_training_interactions\n",
    "\n",
    "df_training_unique_interactions = pd.concat([opt_unique, req_unique])\n",
    "\n",
    "df_pid_trn = pd.merge(df_training_unique_interactions.drop_duplicates(['pid', 'activity']), df_expasy, on='pid', how='inner')\n",
    "df_pid_trn = pd.merge(df_pid_trn, df_profeat, on='pid', how='inner')\n",
    "\n",
    "df_cid_trn = pd.merge(df_training_unique_interactions.drop_duplicates(['cid', 'activity']), df_dragon_features, on='cid', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for types and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid validation set: (7763, 861)\n",
      "cid validation set: (7969, 3645)\n",
      "pid training set: (5919, 861)\n",
      "cid training set: (89513, 3645)\n",
      "pid test set: (897, 859)\n",
      "cid test set: (897, 3643)\n",
      "pid COVID set: (28, 938)\n"
     ]
    }
   ],
   "source": [
    "df_pid_vld.activity = df_pid_vld.activity.astype(float)\n",
    "df_cid_vld.activity = df_cid_vld.activity.astype(float)\n",
    "df_pid_trn.activity = df_pid_trn.activity.astype(float)\n",
    "df_cid_trn.activity = df_cid_trn.activity.astype(float)\n",
    "df_pid_test.activity = df_pid_test.activity.astype(float)\n",
    "df_cid_test.activity = df_cid_test.activity.astype(float)\n",
    "print('pid validation set: {}'.format(df_pid_vld.shape))\n",
    "print('cid validation set: {}'.format(df_cid_vld.shape))\n",
    "print('pid training set: {}'.format(df_pid_trn.shape))\n",
    "print('cid training set: {}'.format(df_cid_trn.shape))\n",
    "print('pid test set: {}'.format(df_pid_test.shape))\n",
    "print('cid test set: {}'.format(df_cid_test.shape))\n",
    "print('pid COVID set: {}'.format(df_covid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Any missing values?\n",
    "print(df_pid_vld.isnull().values.any())\n",
    "print(df_cid_vld.isnull().values.any())\n",
    "print(df_pid_trn.isnull().values.any())\n",
    "print(df_cid_trn.isnull().values.any())\n",
    "print(df_pid_test.isnull().values.any())\n",
    "print(df_cid_test.isnull().values.any())\n",
    "print(df_covid.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    859\n",
       "int64        1\n",
       "object       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pid_vld.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    3597\n",
       "int64        47\n",
       "object        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cid_vld.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    3597\n",
       "int64        47\n",
       "object        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cid_trn.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    859\n",
       "int64        1\n",
       "object       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pid_trn.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('int64'): Index(['cid', 'cid_nAT', 'cid_nSK', 'cid_nTA', 'cid_nBT', 'cid_nBO', 'cid_nBM',\n",
       "        'cid_RBN', 'cid_nDB', 'cid_nTB', 'cid_nAB', 'cid_nH', 'cid_nC',\n",
       "        'cid_nN', 'cid_nO', 'cid_nP', 'cid_nS', 'cid_nF', 'cid_nCL', 'cid_nBR',\n",
       "        'cid_nI', 'cid_nB', 'cid_nHM', 'cid_nHet', 'cid_nX', 'cid_nCsp3',\n",
       "        'cid_nCsp2', 'cid_nCsp', 'cid_nStructures', 'cid_totalcharge',\n",
       "        'cid_nCIC', 'cid_nCIR', 'cid_TRS', 'cid_Rperim', 'cid_Rbrid', 'cid_NRS',\n",
       "        'cid_nR03', 'cid_nR04', 'cid_nR05', 'cid_nR06', 'cid_nR07', 'cid_nR08',\n",
       "        'cid_nR09', 'cid_nR10', 'cid_nR11', 'cid_nR12', 'cid_nBnz'],\n",
       "       dtype='object'),\n",
       " dtype('float64'): Index(['activity', 'cid_binary_weights', 'pid_binary_weights', 'cid_MW',\n",
       "        'cid_AMW', 'cid_Sv', 'cid_Se', 'cid_Sp', 'cid_Si', 'cid_Mv',\n",
       "        ...\n",
       "        'cid_Hy', 'cid_TPSA(NO)', 'cid_TPSA(Tot)', 'cid_SAtot', 'cid_SAacc',\n",
       "        'cid_SAdon', 'cid_Vx', 'cid_VvdwMG', 'cid_VvdwZAZ', 'cid_PDI'],\n",
       "       dtype='object', length=3597),\n",
       " dtype('O'): Index(['pid'], dtype='object')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cid_vld.columns.to_series().groupby(df_cid_vld.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>pid</th>\n",
       "      <th>activity</th>\n",
       "      <th>cid_binary_weights</th>\n",
       "      <th>pid_binary_weights</th>\n",
       "      <th>helical</th>\n",
       "      <th>beta</th>\n",
       "      <th>coil</th>\n",
       "      <th>veryBuried</th>\n",
       "      <th>veryExposed</th>\n",
       "      <th>...</th>\n",
       "      <th>[G7.1.1.71]</th>\n",
       "      <th>[G7.1.1.72]</th>\n",
       "      <th>[G7.1.1.73]</th>\n",
       "      <th>[G7.1.1.74]</th>\n",
       "      <th>[G7.1.1.75]</th>\n",
       "      <th>[G7.1.1.76]</th>\n",
       "      <th>[G7.1.1.77]</th>\n",
       "      <th>[G7.1.1.78]</th>\n",
       "      <th>[G7.1.1.79]</th>\n",
       "      <th>[G7.1.1.80]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>938</td>\n",
       "      <td>AAB59829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952894</td>\n",
       "      <td>0.996703</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>-0.001869</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.002322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986</td>\n",
       "      <td>AAB59829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975912</td>\n",
       "      <td>0.996703</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>-0.001869</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.002322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37542</td>\n",
       "      <td>AAB59829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.963498</td>\n",
       "      <td>0.996703</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>-0.001869</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.002322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>445580</td>\n",
       "      <td>AAB59829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.973330</td>\n",
       "      <td>0.996703</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>-0.001869</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.002322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4100</td>\n",
       "      <td>AAB59829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.923492</td>\n",
       "      <td>0.996703</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>-0.001869</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.002322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cid       pid  activity  cid_binary_weights  pid_binary_weights  \\\n",
       "0     938  AAB59829       1.0            0.952894            0.996703   \n",
       "1    1986  AAB59829       1.0            0.975912            0.996703   \n",
       "2   37542  AAB59829       0.0            0.963498            0.996703   \n",
       "3  445580  AAB59829       0.0            0.973330            0.996703   \n",
       "4    4100  AAB59829       0.0            0.923492            0.996703   \n",
       "\n",
       "   helical   beta   coil  veryBuried  veryExposed  ...  [G7.1.1.71]  \\\n",
       "0    0.349  0.219  0.432       0.326        0.187  ...     0.003074   \n",
       "1    0.349  0.219  0.432       0.326        0.187  ...     0.003074   \n",
       "2    0.349  0.219  0.432       0.326        0.187  ...     0.003074   \n",
       "3    0.349  0.219  0.432       0.326        0.187  ...     0.003074   \n",
       "4    0.349  0.219  0.432       0.326        0.187  ...     0.003074   \n",
       "\n",
       "   [G7.1.1.72]  [G7.1.1.73]  [G7.1.1.74]  [G7.1.1.75]  [G7.1.1.76]  \\\n",
       "0     0.000574    -0.001384    -0.001869    -0.000164    -0.001509   \n",
       "1     0.000574    -0.001384    -0.001869    -0.000164    -0.001509   \n",
       "2     0.000574    -0.001384    -0.001869    -0.000164    -0.001509   \n",
       "3     0.000574    -0.001384    -0.001869    -0.000164    -0.001509   \n",
       "4     0.000574    -0.001384    -0.001869    -0.000164    -0.001509   \n",
       "\n",
       "   [G7.1.1.77]  [G7.1.1.78]  [G7.1.1.79]  [G7.1.1.80]  \n",
       "0     0.000604    -0.000104     0.001366     0.002322  \n",
       "1     0.000604    -0.000104     0.001366     0.002322  \n",
       "2     0.000604    -0.000104     0.001366     0.002322  \n",
       "3     0.000604    -0.000104     0.001366     0.002322  \n",
       "4     0.000604    -0.000104     0.001366     0.002322  \n",
       "\n",
       "[5 rows x 861 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pid_vld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>cid</th>\n",
       "      <th>cid_binary_weights</th>\n",
       "      <th>pid</th>\n",
       "      <th>pid_binary_weights</th>\n",
       "      <th>helical</th>\n",
       "      <th>beta</th>\n",
       "      <th>coil</th>\n",
       "      <th>veryBuried</th>\n",
       "      <th>veryExposed</th>\n",
       "      <th>...</th>\n",
       "      <th>[G7.1.1.71]</th>\n",
       "      <th>[G7.1.1.72]</th>\n",
       "      <th>[G7.1.1.73]</th>\n",
       "      <th>[G7.1.1.74]</th>\n",
       "      <th>[G7.1.1.75]</th>\n",
       "      <th>[G7.1.1.76]</th>\n",
       "      <th>[G7.1.1.77]</th>\n",
       "      <th>[G7.1.1.78]</th>\n",
       "      <th>[G7.1.1.79]</th>\n",
       "      <th>[G7.1.1.80]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAA96025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>-0.000553</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.005002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAA96025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>-0.000553</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.005002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAC83551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.003536</td>\n",
       "      <td>-0.005011</td>\n",
       "      <td>-0.003009</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.004730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>44093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAC83551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.003536</td>\n",
       "      <td>-0.005011</td>\n",
       "      <td>-0.003009</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.004730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADQ57959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012397</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.013820</td>\n",
       "      <td>0.006126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   activity    cid  cid_binary_weights       pid  pid_binary_weights  helical  \\\n",
       "0       0.0  38258                 NaN  CAA96025                 NaN    0.595   \n",
       "1       1.0    564                 NaN  CAA96025                 NaN    0.595   \n",
       "2       0.0  38258                 NaN  AAC83551                 NaN    0.344   \n",
       "3       1.0  44093                 NaN  AAC83551                 NaN    0.344   \n",
       "4       0.0  38258                 NaN  ADQ57959                 NaN    0.677   \n",
       "\n",
       "    beta   coil  veryBuried  veryExposed  ...  [G7.1.1.71]  [G7.1.1.72]  \\\n",
       "0  0.000  0.405       0.513        0.196  ...     0.000589     0.003648   \n",
       "1  0.000  0.405       0.513        0.196  ...     0.000589     0.003648   \n",
       "2  0.153  0.503       0.257        0.274  ...     0.001158    -0.000115   \n",
       "3  0.153  0.503       0.257        0.274  ...     0.001158    -0.000115   \n",
       "4  0.008  0.315       0.027        0.714  ...     0.012397     0.005213   \n",
       "\n",
       "   [G7.1.1.73]  [G7.1.1.74]  [G7.1.1.75]  [G7.1.1.76]  [G7.1.1.77]  \\\n",
       "0    -0.000553     0.001793     0.002062     0.005031     0.002675   \n",
       "1    -0.000553     0.001793     0.002062     0.005031     0.002675   \n",
       "2    -0.003536    -0.005011    -0.003009     0.001358     0.003703   \n",
       "3    -0.003536    -0.005011    -0.003009     0.001358     0.003703   \n",
       "4     0.009940     0.004749     0.013094     0.005662     0.010197   \n",
       "\n",
       "   [G7.1.1.78]  [G7.1.1.79]  [G7.1.1.80]  \n",
       "0     0.005840     0.001753     0.005002  \n",
       "1     0.005840     0.001753     0.005002  \n",
       "2     0.002481     0.003668     0.004730  \n",
       "3     0.002481     0.003668     0.004730  \n",
       "4     0.005612     0.013820     0.006126  \n",
       "\n",
       "[5 rows x 861 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pid_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>cid</th>\n",
       "      <th>cid_binary_weights</th>\n",
       "      <th>pid</th>\n",
       "      <th>pid_binary_weights</th>\n",
       "      <th>cid_MW</th>\n",
       "      <th>cid_AMW</th>\n",
       "      <th>cid_Sv</th>\n",
       "      <th>cid_Se</th>\n",
       "      <th>cid_Sp</th>\n",
       "      <th>...</th>\n",
       "      <th>cid_Hy</th>\n",
       "      <th>cid_TPSA(NO)</th>\n",
       "      <th>cid_TPSA(Tot)</th>\n",
       "      <th>cid_SAtot</th>\n",
       "      <th>cid_SAacc</th>\n",
       "      <th>cid_SAdon</th>\n",
       "      <th>cid_Vx</th>\n",
       "      <th>cid_VvdwMG</th>\n",
       "      <th>cid_VvdwZAZ</th>\n",
       "      <th>cid_PDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAA96025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>388.35</td>\n",
       "      <td>9.031</td>\n",
       "      <td>26.997</td>\n",
       "      <td>45.967</td>\n",
       "      <td>25.727</td>\n",
       "      <td>...</td>\n",
       "      <td>5.553</td>\n",
       "      <td>221.54</td>\n",
       "      <td>221.54</td>\n",
       "      <td>472.908</td>\n",
       "      <td>350.549</td>\n",
       "      <td>193.431</td>\n",
       "      <td>407.193</td>\n",
       "      <td>170.259</td>\n",
       "      <td>316.62</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5281718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAA96025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>390.42</td>\n",
       "      <td>7.808</td>\n",
       "      <td>31.513</td>\n",
       "      <td>51.338</td>\n",
       "      <td>32.011</td>\n",
       "      <td>...</td>\n",
       "      <td>3.356</td>\n",
       "      <td>139.84</td>\n",
       "      <td>139.84</td>\n",
       "      <td>563.698</td>\n",
       "      <td>278.100</td>\n",
       "      <td>256.100</td>\n",
       "      <td>460.033</td>\n",
       "      <td>191.915</td>\n",
       "      <td>347.52</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5281718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P08659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>390.42</td>\n",
       "      <td>7.808</td>\n",
       "      <td>31.513</td>\n",
       "      <td>51.338</td>\n",
       "      <td>32.011</td>\n",
       "      <td>...</td>\n",
       "      <td>3.356</td>\n",
       "      <td>139.84</td>\n",
       "      <td>139.84</td>\n",
       "      <td>563.698</td>\n",
       "      <td>278.100</td>\n",
       "      <td>256.100</td>\n",
       "      <td>460.033</td>\n",
       "      <td>191.915</td>\n",
       "      <td>347.52</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>443936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAA96025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>508.61</td>\n",
       "      <td>7.266</td>\n",
       "      <td>42.253</td>\n",
       "      <td>71.221</td>\n",
       "      <td>43.762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>106.97</td>\n",
       "      <td>106.97</td>\n",
       "      <td>697.173</td>\n",
       "      <td>195.183</td>\n",
       "      <td>42.683</td>\n",
       "      <td>609.070</td>\n",
       "      <td>252.996</td>\n",
       "      <td>484.05</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>443936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P08684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>508.61</td>\n",
       "      <td>7.266</td>\n",
       "      <td>42.253</td>\n",
       "      <td>71.221</td>\n",
       "      <td>43.762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>106.97</td>\n",
       "      <td>106.97</td>\n",
       "      <td>697.173</td>\n",
       "      <td>195.183</td>\n",
       "      <td>42.683</td>\n",
       "      <td>609.070</td>\n",
       "      <td>252.996</td>\n",
       "      <td>484.05</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3645 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   activity      cid  cid_binary_weights       pid  pid_binary_weights  \\\n",
       "0       0.0    38258                 NaN  CAA96025                 NaN   \n",
       "1       0.0  5281718                 NaN  CAA96025                 NaN   \n",
       "2       1.0  5281718                 NaN    P08659                 NaN   \n",
       "3       0.0   443936                 NaN  CAA96025                 NaN   \n",
       "4       1.0   443936                 NaN    P08684                 NaN   \n",
       "\n",
       "   cid_MW  cid_AMW  cid_Sv  cid_Se  cid_Sp  ...  cid_Hy  cid_TPSA(NO)  \\\n",
       "0  388.35    9.031  26.997  45.967  25.727  ...   5.553        221.54   \n",
       "1  390.42    7.808  31.513  51.338  32.011  ...   3.356        139.84   \n",
       "2  390.42    7.808  31.513  51.338  32.011  ...   3.356        139.84   \n",
       "3  508.61    7.266  42.253  71.221  43.762  ...  -0.355        106.97   \n",
       "4  508.61    7.266  42.253  71.221  43.762  ...  -0.355        106.97   \n",
       "\n",
       "   cid_TPSA(Tot)  cid_SAtot  cid_SAacc  cid_SAdon   cid_Vx  cid_VvdwMG  \\\n",
       "0         221.54    472.908    350.549    193.431  407.193     170.259   \n",
       "1         139.84    563.698    278.100    256.100  460.033     191.915   \n",
       "2         139.84    563.698    278.100    256.100  460.033     191.915   \n",
       "3         106.97    697.173    195.183     42.683  609.070     252.996   \n",
       "4         106.97    697.173    195.183     42.683  609.070     252.996   \n",
       "\n",
       "   cid_VvdwZAZ  cid_PDI  \n",
       "0       316.62    0.861  \n",
       "1       347.52    0.816  \n",
       "2       347.52    0.816  \n",
       "3       484.05    0.874  \n",
       "4       484.05    0.874  \n",
       "\n",
       "[5 rows x 3645 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cid_trn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LDA\n",
    "\n",
    "Gaussian Mixture Models are not working well with our data so we'll try LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# validation weighting\\n# train LDA for validation weights\\nvalidation_interactions = pd.read_csv(validation_interactions_csv, index_col=0)\\nvalidation_interactions.drop(\"latent_prob_delta_ratio\", axis=1, inplace=True)\\nrequired_training_interactions = pd.read_csv(required_training_interactions_csv, index_col=0)\\noptional_training_interactions = pd.read_csv(optional_training_interactions_csv, index_col=0)\\n\\nopt_unique = optional_training_interactions.drop_duplicates([\"cid\", \"activity\"]).drop(\"activity_score\", axis=1)\\nreq_unique = required_training_interactions.drop_duplicates([\"cid\", \"activity\"])\\n\\ntraining_unique = pd.concat([opt_unique, req_unique]).drop_duplicates([\"cid\", \"activity\"])\\nnum_topics = 100\\nlda = LatentDirichletAllocation(n_components=num_topics, random_state=42, learning_method=\"online\", n_jobs=-1)\\n\\ncid_fingerprints_file = \"dataset_raw_files/fingerprints.csv\"\\ncid_fingerprints = pd.read_csv(cid_fingerprints_file)\\ncid_interactions = cid_fingerprints.merge(training_unique.drop(\"pid\", axis=1), on=\"cid\").drop(\"cid\", axis=1)\\n\\nlda.fit(cid_interactions)\\ndel cid_interactions\\ncid_fingerprints[\"pos\"] = 1\\ncid_fingerprints[\"neg\"] = 0\\ncid_fingerprints.set_index(\"cid\", inplace=True)\\n\\nlatent_prob_pos = np.max(lda.transform(cid_fingerprints.drop(\"neg\", axis=1)), axis=1)\\ncid_fingerprints[\"latent_prob_neg\"] = np.max(lda.transform(cid_fingerprints.drop(\"pos\", axis=1)), axis=1)\\ncid_fingerprints.reset_index(inplace=True)\\ncid_fingerprints[\"latent_prob_pos\"] = latent_prob_pos\\ncid_fingerprints.drop([\"pos\", \"neg\"], axis=1, inplace=True)\\ncid_fingerprints[\"latent_prob_delta\"] = np.abs(cid_fingerprints.latent_prob_pos - cid_fingerprints.latent_prob_neg)\\ncid_fingerprints[\"latent_prob_delta_ratio\"] = 1 - 2 * cid_fingerprints.latent_prob_delta / (cid_fingerprints.latent_prob_pos + cid_fingerprints.latent_prob_neg)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reference: Brian's code for doing a similiar thing on the binary features using LDA:\n",
    "\n",
    "'''\n",
    "# validation weighting\n",
    "# train LDA for validation weights\n",
    "validation_interactions = pd.read_csv(validation_interactions_csv, index_col=0)\n",
    "validation_interactions.drop(\"latent_prob_delta_ratio\", axis=1, inplace=True)\n",
    "required_training_interactions = pd.read_csv(required_training_interactions_csv, index_col=0)\n",
    "optional_training_interactions = pd.read_csv(optional_training_interactions_csv, index_col=0)\n",
    "\n",
    "opt_unique = optional_training_interactions.drop_duplicates([\"cid\", \"activity\"]).drop(\"activity_score\", axis=1)\n",
    "req_unique = required_training_interactions.drop_duplicates([\"cid\", \"activity\"])\n",
    "\n",
    "training_unique = pd.concat([opt_unique, req_unique]).drop_duplicates([\"cid\", \"activity\"])\n",
    "num_topics = 100\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42, learning_method=\"online\", n_jobs=-1)\n",
    "\n",
    "cid_fingerprints_file = \"dataset_raw_files/fingerprints.csv\"\n",
    "cid_fingerprints = pd.read_csv(cid_fingerprints_file)\n",
    "cid_interactions = cid_fingerprints.merge(training_unique.drop(\"pid\", axis=1), on=\"cid\").drop(\"cid\", axis=1)\n",
    "\n",
    "lda.fit(cid_interactions)\n",
    "del cid_interactions\n",
    "cid_fingerprints[\"pos\"] = 1\n",
    "cid_fingerprints[\"neg\"] = 0\n",
    "cid_fingerprints.set_index(\"cid\", inplace=True)\n",
    "\n",
    "latent_prob_pos = np.max(lda.transform(cid_fingerprints.drop(\"neg\", axis=1)), axis=1)\n",
    "cid_fingerprints[\"latent_prob_neg\"] = np.max(lda.transform(cid_fingerprints.drop(\"pos\", axis=1)), axis=1)\n",
    "cid_fingerprints.reset_index(inplace=True)\n",
    "cid_fingerprints[\"latent_prob_pos\"] = latent_prob_pos\n",
    "cid_fingerprints.drop([\"pos\", \"neg\"], axis=1, inplace=True)\n",
    "cid_fingerprints[\"latent_prob_delta\"] = np.abs(cid_fingerprints.latent_prob_pos - cid_fingerprints.latent_prob_neg)\n",
    "cid_fingerprints[\"latent_prob_delta_ratio\"] = 1 - 2 * cid_fingerprints.latent_prob_delta / (cid_fingerprints.latent_prob_pos + cid_fingerprints.latent_prob_neg)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-feature cols\n",
    "pids_vld = df_pid_vld.drop(['cid','pid','cid_binary_weights','pid_binary_weights'], axis=1)\n",
    "cids_vld = df_cid_vld.drop(['cid','pid','cid_binary_weights','pid_binary_weights'], axis=1)\n",
    "\n",
    "pids_trn = df_pid_trn.drop(['cid','pid','cid_binary_weights','pid_binary_weights'], axis=1)\n",
    "cids_trn = df_cid_trn.drop(['cid','pid','cid_binary_weights','pid_binary_weights'], axis=1)\n",
    "\n",
    "pids_test = df_pid_test.drop(['cid','pid'], axis=1)\n",
    "cids_test = df_cid_test.drop(['cid','pid'], axis=1)\n",
    "\n",
    "covid = df_covid.drop(['pid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Any missing values now?\n",
    "print(pids_vld.isnull().values.any())\n",
    "print(cids_vld.isnull().values.any())\n",
    "\n",
    "print(pids_trn.isnull().values.any())\n",
    "print(cids_trn.isnull().values.any())\n",
    "\n",
    "print(pids_test.isnull().values.any())\n",
    "print(cids_test.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid validation set: (7763, 857)\n",
      "cid validation set: (7969, 3641)\n",
      "pid training set: (5919, 857)\n",
      "cid training set: (89513, 3641)\n",
      "pid test set: (897, 857)\n",
      "cid test set: (897, 3641)\n",
      "pid COVID set: (28, 938)\n"
     ]
    }
   ],
   "source": [
    "print('pid validation set: {}'.format(pids_vld.shape))\n",
    "print('cid validation set: {}'.format(cids_vld.shape))\n",
    "\n",
    "print('pid training set: {}'.format(pids_trn.shape))\n",
    "print('cid training set: {}'.format(cids_trn.shape))\n",
    "\n",
    "print('pid test set: {}'.format(pids_test.shape))\n",
    "print('cid test set: {}'.format(cids_test.shape))\n",
    "\n",
    "print('pid COVID set: {}'.format(df_covid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 268 columns with negative values in the cid validation set.\n"
     ]
    }
   ],
   "source": [
    "# Looks for negative values\n",
    "print('There are {} columns with negative values in the cid validation set.'\n",
    "      .format(len(cids_vld.columns[(cids_vld < 0).any()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous features scaler load/save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cid_scaler.jl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "pid_scaler_trn = MinMaxScaler()\n",
    "pids_trn[pids_trn.columns] = pid_scaler_trn.fit_transform(pids_trn)\n",
    "cid_scaler_trn = MinMaxScaler()\n",
    "cids_trn[cids_trn.columns] = cid_scaler_trn.fit_transform(cids_trn)\n",
    "\n",
    "joblib.dump(pid_scaler_trn, 'pid_scaler.jl')\n",
    "joblib.dump(cid_scaler_trn, 'cid_scaler.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_scaler_trn = joblib.load('pid_scaler.jl')\n",
    "cid_scaler_trn = joblib.load('cid_scaler.jl')\n",
    "\n",
    "pids_trn[pids_trn.columns] = pid_scaler_trn.transform(pids_trn)\n",
    "cids_trn[cids_trn.columns] = cid_scaler_trn.transform(cids_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "0.0 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(pids_trn['[G7.1.1.75]'].min(), pids_trn['[G7.1.1.75]'].max())\n",
    "print(cids_trn['cid_AMW'].min(), cids_trn['cid_AMW'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scale the validation data using the MinMaxScaler fitted on the training data.\n",
    "'''\n",
    "pids_vld[pids_vld.columns] = pid_scaler_trn.transform(pids_vld)\n",
    "cids_vld[cids_vld.columns] = cid_scaler_trn.transform(cids_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 columns with negative values in the pid validation set.\n",
      "There are 0 columns with negative values in the pid validation set.\n"
     ]
    }
   ],
   "source": [
    "# Looks for negative values in the scaled validation data\n",
    "print('There are {} columns with negative values in the pid validation set.'\n",
    "      .format(len(pids_vld.columns[(pids_vld < 0).any()])))\n",
    "\n",
    "print('There are {} columns with negative values in the pid validation set.'\n",
    "      .format(len(cids_vld.columns[(cids_vld < 0).any()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scale the test data using the MinMaxScaler fitted on the training data.\n",
    "'''\n",
    "pids_test[pids_test.columns] = pid_scaler_trn.transform(pids_test)\n",
    "cids_test[cids_test.columns] = cid_scaler_trn.transform(cids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns with negative values in the pid test set.\n",
      "There are 0 columns with negative values in the cid test set.\n"
     ]
    }
   ],
   "source": [
    "# Looks for negative values in the scaled validation data\n",
    "print('There are {} columns with negative values in the pid test set.'\n",
    "      .format(len(pids_test.columns[(pids_test < 0).any()])))\n",
    "\n",
    "print('There are {} columns with negative values in the cid test set.'\n",
    "      .format(len(cids_test.columns[(cids_test < 0).any()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scale the COVID data using the MinMaxScaler fitted on the training data.\n",
    "'''\n",
    "covid[covid.columns] = pid_scaler_trn.transform(covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks for negative values in the scaled COVID data\n",
    "print('There are {} columns with negative values in the COVID test set.'\n",
    "      .format(len(covid.columns[(covid < 0).any()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some of the pid validation columns contain -ve values. Find the minimum value and add \n",
    "it to each value of the affected column in the validation, test\n",
    "and training sets.\n",
    "'''\n",
    "neg_cols = pids_vld.columns[(pids_vld < 0).any()]\n",
    "min_vals = pids_vld[neg_cols].min().values\n",
    "\n",
    "for idx, col in enumerate(neg_cols):\n",
    "    pids_vld[col] -=  min_vals[idx]\n",
    "    pids_trn[col] -=  min_vals[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_vld[neg_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There should now be no -ve values\n",
    "print('There are {} columns with negative values in the pid validation set.'\n",
    "      .format(len(pids_vld.columns[(pids_vld < 0).any()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks for negative values in the scaled validation data\n",
    "print('There are {} columns with negative values in the cid validation set.'\n",
    "      .format(len(cids_vld.columns[(cids_vld < 0).any()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "num_topics = 100\n",
    "\n",
    "def LDA_fit(df_in, num_topics):\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, \n",
    "                                        random_state=42, learning_method=\"online\", n_jobs=-1)\n",
    "    lda.fit(df_in)\n",
    "\n",
    "    return lda\n",
    "\n",
    "\n",
    "def LDA_transform(df_in, model):\n",
    "    df_in[\"activity\"] = 1\n",
    "    latent_prob_pos = np.max(model.transform(df_in), axis=1)\n",
    "    \n",
    "    df_in[\"activity\"] = 0\n",
    "    df_in[\"latent_prob_neg\"] = np.max(model.transform(df_in), axis=1)\n",
    "    \n",
    "    df_in[\"latent_prob_pos\"] = latent_prob_pos\n",
    "    df_in[\"latent_prob_delta\"] = np.abs(df_in.latent_prob_pos - df_in.latent_prob_neg)\n",
    "    df_in[\"latent_prob_delta_ratio\"] = 1 - 2 * df_in.latent_prob_delta / (df_in.latent_prob_pos + df_in.latent_prob_neg)\n",
    "    \n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LDA on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cid = LDA_fit(cids_trn, num_topics)\n",
    "lda_pid = LDA_fit(pids_trn, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "joblib.dump(lda_cid, 'lda_cid.jl')\n",
    "joblib.dump(lda_pid, 'lda_pid.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cid = joblib.load('lda_cid.jl')\n",
    "lda_pid = joblib.load('lda_pid.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LDA weights for the continuous features, split across optional and required files.\n",
    "optional_training_interactions_csv = '../data/v5/optional_training_interactions_v5.csv'\n",
    "required_training_interactions_csv = '../data/v5/required_training_interactions_v5.csv'\n",
    "\n",
    "optional_training_interactions = pd.read_csv(optional_training_interactions_csv, index_col=0)\n",
    "required_training_interactions = pd.read_csv(required_training_interactions_csv, index_col=0)\n",
    "\n",
    "df_opt_pid_training = pd.merge(optional_training_interactions.drop_duplicates(['pid', 'activity']), df_expasy, on='pid', how='inner')\n",
    "df_opt_pid_training = pd.merge(df_opt_pid_training, df_profeat, on='pid', how='inner')\n",
    "\n",
    "df_req_pid_training = pd.merge(required_training_interactions.drop_duplicates(['pid', 'activity']), df_expasy, on='pid', how='inner')\n",
    "df_req_pid_training = pd.merge(df_req_pid_training, df_profeat, on='pid', how='inner')\n",
    "\n",
    "df_opt_cid_training = pd.merge(optional_training_interactions.drop_duplicates(['cid', 'activity']), df_dragon_features, on='cid', how='inner')\n",
    "df_req_cid_training = pd.merge(required_training_interactions.drop_duplicates(['cid', 'activity']), df_dragon_features, on='cid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157531 21704\n",
      "5235 684 5919\n",
      "81795 13383 95178\n"
     ]
    }
   ],
   "source": [
    "print(len(optional_training_interactions), len(required_training_interactions))\n",
    "print(len(df_opt_pid_training), len(df_req_pid_training), len(df_opt_pid_training) + len(df_req_pid_training))\n",
    "print(len(df_opt_cid_training), len(df_req_cid_training), len(df_opt_cid_training) + len(df_req_cid_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optional_training_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opt_pid_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opt_cid_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_training_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req_pid_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req_cid_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns with negative values in the cid optional set.\n"
     ]
    }
   ],
   "source": [
    "# fit LDA to the training set.\n",
    "opt_pid_cols = df_opt_pid_training.columns\n",
    "opt_cid_cols = df_opt_cid_training.columns\n",
    "req_pid_cols = df_req_pid_training.columns\n",
    "req_cid_cols = df_req_cid_training.columns\n",
    "\n",
    "opt_drop_cols = ['cid_binary_weights', 'pid_binary_weights', 'pid', 'cid', 'activity_score']\n",
    "req_drop_cols = ['cid_binary_weights', 'pid_binary_weights', 'pid', 'cid']\n",
    "\n",
    "df_opt_cid_training_in = df_opt_cid_training[opt_cid_cols].drop(opt_drop_cols, axis=1).copy()\n",
    "df_req_cid_training_in = df_req_cid_training[req_cid_cols].drop(req_drop_cols, axis=1).copy()\n",
    "\n",
    "df_opt_pid_training_in = df_opt_pid_training[opt_pid_cols].drop(opt_drop_cols, axis=1).copy()\n",
    "df_req_pid_training_in = df_req_pid_training[req_pid_cols].drop(req_drop_cols, axis=1).copy()\n",
    "\n",
    "# scale\n",
    "df_opt_cid_training_in[df_opt_cid_training_in.columns] = cid_scaler_trn.transform(df_opt_cid_training_in)\n",
    "df_req_cid_training_in[df_req_cid_training_in.columns] = cid_scaler_trn.transform(df_req_cid_training_in)\n",
    "\n",
    "df_opt_pid_training_in[df_opt_pid_training_in.columns] = pid_scaler_trn.transform(df_opt_pid_training_in)\n",
    "df_req_pid_training_in[df_req_pid_training_in.columns] = pid_scaler_trn.transform(df_req_pid_training_in)\n",
    "\n",
    "# Looks for negative values in the scaled validation data\n",
    "print('There are {} columns with negative values in the cid optional set.'\n",
    "  .format(len(df_opt_cid_training_in.columns[(df_opt_cid_training_in < 0).any()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.019458 0.017741\n",
      "-0.01725 0.017741\n",
      "-0.019458 0.013636\n"
     ]
    }
   ],
   "source": [
    "print(df_pid_trn['[G7.1.1.75]'].min(), df_pid_trn['[G7.1.1.75]'].max())\n",
    "print(df_opt_pid_training['[G7.1.1.75]'].min(), df_opt_pid_training['[G7.1.1.75]'].max())\n",
    "print(df_req_pid_training['[G7.1.1.75]'].min(), df_req_pid_training['[G7.1.1.75]'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059356434312750284 1.0\n",
      "0.0 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(df_opt_pid_training_in['[G7.1.1.75]'].min(), df_opt_pid_training_in['[G7.1.1.75]'].max())\n",
    "print(df_opt_cid_training_in['cid_AMW'].min(), df_opt_cid_training_in['cid_AMW'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjust pid columns that were rescaled to avoid -ve validation values.\n",
    "'''\n",
    "neg_cols = pids_vld.columns[(pids_vld < 0).any()]\n",
    "min_vals = pids_vld[neg_cols].min().values\n",
    "\n",
    "for idx, col in enumerate(neg_cols):\n",
    "    df_opt_pid_training_in[col] -=  min_vals[idx]\n",
    "    df_req_pid_training_in[col] -=  min_vals[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "df_opt_cid_training_in = LDA_transform(df_opt_cid_training_in, lda_cid)\n",
    "df_req_cid_training_in = LDA_transform(df_req_cid_training_in, lda_cid)\n",
    "\n",
    "df_opt_pid_training_in = LDA_transform(df_opt_pid_training_in, lda_pid)\n",
    "df_req_pid_training_in = LDA_transform(df_req_pid_training_in, lda_pid)\n",
    "\n",
    "\n",
    "# Add the new LDA weight columns to the full training set.\n",
    "df_opt_cid_training_in['cid_continuous_weights'] = df_opt_cid_training_in['latent_prob_delta_ratio']\n",
    "df_req_cid_training_in['cid_continuous_weights'] = df_req_cid_training_in['latent_prob_delta_ratio']\n",
    "\n",
    "df_opt_pid_training_in['pid_continuous_weights'] = df_opt_pid_training_in['latent_prob_delta_ratio']\n",
    "df_req_pid_training_in['pid_continuous_weights'] = df_req_pid_training_in['latent_prob_delta_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the pid and CID back to the data\n",
    "df_opt_cid_training_in['cid'] = df_opt_cid_training['cid']\n",
    "df_req_cid_training_in['cid'] = df_req_cid_training['cid']\n",
    "\n",
    "df_opt_pid_training_in['pid'] = df_opt_pid_training['pid']\n",
    "df_req_pid_training_in['pid']= df_req_pid_training['pid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_opt_cid_training_in['cid_continuous_weights'].min(), df_opt_cid_training_in['cid_continuous_weights'].max())\n",
    "print(df_opt_pid_training_in['pid_continuous_weights'].min(), df_opt_pid_training_in['pid_continuous_weights'].max())\n",
    "\n",
    "print(df_req_cid_training_in['cid_continuous_weights'].min(), df_req_cid_training_in['cid_continuous_weights'].max())\n",
    "print(df_req_pid_training_in['pid_continuous_weights'].min(), df_req_pid_training_in['pid_continuous_weights'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for the v6 validation files\n",
    "\n",
    "These are just the v5 files with the LDA weights for continuous fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "v6_path = '../data/v6'\n",
    "try:\n",
    "  os.makedirs(v6_path, exist_ok=True)\n",
    "except OSError:\n",
    "  print(\"Creation of the directory %s failed\" % v6_path)\n",
    "else:\n",
    "  print(\"Successfully created the directory %s \" % v6_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the LDA-based weights for the continuous features to the matched interactions and save to v6\n",
    "\n",
    "optional_training_interactions = pd.merge(optional_training_interactions, df_opt_cid_training_in, on='cid', how='inner')\n",
    "optional_training_interactions = pd.merge(optional_training_interactions, df_opt_pid_training_in, on='pid', how='inner')\n",
    "required_training_interactions = pd.merge(required_training_interactions, df_req_cid_training_in, on='cid', how='inner')\n",
    "required_training_interactions = pd.merge(required_training_interactions, df_req_pid_training_in, on='pid', how='inner')\n",
    "\n",
    "optional_training_interactions.to_csv(v6_path+'/optional_training_interactions_v6.csv', index=False)\n",
    "required_training_interactions.to_csv(v6_path+'/required_training_interactions_v6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optional_training_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_training_interactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cids_vld = LDA_transform(cids_vld, lda_cid)\n",
    "pids_vld = LDA_transform(pids_vld, lda_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cids_vld['latent_prob_delta_ratio'].min(), cids_vld['latent_prob_delta_ratio'].max())\n",
    "print(pids_vld['latent_prob_delta_ratio'].min(), pids_vld['latent_prob_delta_ratio'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cids_vld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_vld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_interactions = '../data/v5/validation_interactions_v5.csv'\n",
    "df_validation_interactions = __load_data(validation_interactions)\n",
    "\n",
    "# Add the two new column for the continuous LDA weights for cids and pids to create the v6 validation file\n",
    "df_validation_interactions['cid_continuous_weights'] = cids_vld['latent_prob_delta_ratio']\n",
    "df_validation_interactions['pid_continuous_weights'] = pids_vld['latent_prob_delta_ratio']\n",
    "\n",
    "# Save the new weights to the validation interactions file.\n",
    "df_validation_interactions.to_csv(v6_path+'/validation_interactions_v6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to another file for potential future use\n",
    "# Add the new latent_prob_delta_ratio to the original dataframes and save to csv.\n",
    "new_cols = ['latent_prob_delta_ratio', 'latent_prob_neg', 'latent_prob_pos', 'latent_prob_delta']\n",
    "df_pid_vld[new_cols] = pids_vld[new_cols]\n",
    "df_cid_vld[new_cols] = cids_vld[new_cols]\n",
    "\n",
    "df_pid_vld[['cid', 'pid']+new_cols].to_csv('../data/pid_v5_LDA_continuous.csv', index=False)\n",
    "df_cid_vld[['cid', 'pid']+new_cols].to_csv('../data/cid_v5_LDA_continuous.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cids_vld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_vld.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a GMM over each feature set\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">None of the stuff below worked</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_pid = GaussianMixture(n_components=50,\n",
    "              covariance_type='full', max_iter=50, random_state=42, reg_covar=0.000001)\n",
    "\n",
    "gmm_cid = GaussianMixture(n_components=50,\n",
    "              covariance_type='full', max_iter=50, random_state=42, reg_covar=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Projection to reduce dimensionality before training GMM\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">GMM predict_proba was only returning binary values for probabilties so I tried RP for dimension reduction to see if this improved things....it didn't.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "transformer = SparseRandomProjection(random_state=rng, eps=0.2)\n",
    "cids_trn = transformer.fit_transform(cids_trn)\n",
    "print(cids_trn.shape)\n",
    "\n",
    "transformer = SparseRandomProjection(random_state=rng, eps=0.4)\n",
    "pids_trn = transformer.fit_transform(pids_trn)\n",
    "print(pids.shape)\n",
    "\n",
    "# very few components are non-zero\n",
    "np.mean(transformer.components_ != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the GMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_cid.fit(cids_trn)\n",
    "gmm_cid.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_pid.fit(pids_trn)\n",
    "gmm_pid.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_cid.predict_proba([cids_trn[23]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_pid.predict_proba([pids_trn[45]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain likelhood of cluster membership from multivariate normal pdf\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">Maybe we can get non-binary probabilities by looking at the multivaraite pdf for a point's cluster? Nope, we can't, in this case. The pdf always just returns `inf`. </span>\n",
    "\n",
    "See https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def p(model, v):\n",
    "    p = model.predict_proba([v])\n",
    "    cluster = np.argmax(p)\n",
    "    mu = model.means_[cluster]\n",
    "    cov = model.covariances_[cluster]\n",
    "    return multivariate_normal.pdf(v, mean=mu, cov=cov)\n",
    "\n",
    "p(gmm_pid, pids_trn[2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis distance differences when flipping activity\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">Maybe the Mahalanobis distance can be used? Nope, doesn't look lke it. The scores look off.</span>\n",
    "\n",
    "For each point, obtain a prediction from GMM of its closest cluster. Then calculate the mahalanobis distance to its  cluster. Next, flip the activity bit and re-predict and calculate distance again. Finally, calculate the poiint's score given the change in distance. NOTE: this assumes that flipping activity does not change cluster membership, only likelihood of membership of the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def dist(model, v):\n",
    "    p = model.predict_proba(v)\n",
    "    cluster = np.argmax(p)\n",
    "    mu = model.means_[cluster]\n",
    "    cov = model.covariances_[cluster]\n",
    "    icov = np.linalg.inv(cov) \n",
    "    return distance.mahalanobis(v, mu, icov)\n",
    "\n",
    "def get_score(vin, model):\n",
    "    v = [vin]\n",
    "    d1 = dist(model, v)\n",
    "    print(d1)\n",
    "\n",
    "    # Flip activity and measure distance\n",
    "    if vin[0] == 1.0:\n",
    "        vin[0] = 0.0\n",
    "    else:\n",
    "        vin[0] = 1.0\n",
    "\n",
    "    d2 = dist(model, v)\n",
    "    print(d2)\n",
    "    \n",
    "    score = 1 - 2 * np.abs(d1-d2)/d1+d2\n",
    "    return score\n",
    "    \n",
    "pid_scores = [get_score(v, gmm_pid) for v in pids_trn[:10]] # Try on the first 10 pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-L divergence\n",
    "\n",
    "<span style=\"color:red; font-weight:bold;\">Never got to try this since we couldn't get good results from GMM.</span>\n",
    "\n",
    "Since there is no closed-form solution over GMMs we will use an approximation based upon Monte Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_kl(gmm_p, gmm_q, n_samples=10**5):\n",
    "    X = gmm_p.sample(n_samples)\n",
    "    log_p_X, _ = gmm_p.score_samples(X)\n",
    "    log_q_X, _ = gmm_q.score_samples(X)\n",
    "    return log_p_X.mean() - log_q_X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
