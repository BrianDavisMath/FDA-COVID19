{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation-based feature selection\n",
    "\n",
    "This notebook takes the outptut of **centroid-sampling.pynb** and removes columns that are highly correlated (_>90%_)with other columns.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "* identify the columns from each sub data set, e.g. fingerprints and binding-sites\n",
    "* extract a subset, consiting of those columns from the centroid-sampled set (this limits cardinality)\n",
    "* use the _Deepgraph_ code to  measure pairwise correlations across a random sample of 1,000 rows, across those columns. Retain the highly correlated column pairs.\n",
    "* calculate pairwise correlation across all rows for the retained columns\n",
    "* take one column from each remaining pair that are highly correlated\n",
    "* remove all remaining highly-correlated columns from the data set and save back to file\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%autosave 25\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import deepgraph as dg\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = '../data/FDA-COVID19_files_v1.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data set output from centroid-sampling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 22,172, columns: 16,391\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>pid</th>\n",
       "      <th>activity</th>\n",
       "      <th>AEK</th>\n",
       "      <th>VEL</th>\n",
       "      <th>EKF</th>\n",
       "      <th>LGM</th>\n",
       "      <th>VKN</th>\n",
       "      <th>LKP</th>\n",
       "      <th>NEE</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>204</td>\n",
       "      <td>Q99VQ4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>204</td>\n",
       "      <td>EDT84149</td>\n",
       "      <td>0</td>\n",
       "      <td>3.936758</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>204</td>\n",
       "      <td>AAX80043</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>204</td>\n",
       "      <td>P0C6X7</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.367871</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>8549</td>\n",
       "      <td>P08659</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 16391 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cid       pid  activity       AEK  VEL  EKF  LGM  VKN  LKP       NEE  \\\n",
       "58    204    Q99VQ4         1 -1.000000 -1.0 -1.0 -1.0 -1.0 -1.0 -1.000000   \n",
       "97    204  EDT84149         0  3.936758 -1.0 -1.0 -1.0 -1.0 -1.0 -1.000000   \n",
       "98    204  AAX80043         0 -1.000000 -1.0 -1.0 -1.0 -1.0 -1.0 -1.000000   \n",
       "126   204    P0C6X7         1 -1.000000 -1.0 -1.0 -1.0 -1.0 -1.0  2.367871   \n",
       "136  8549    P08659         0 -1.000000 -1.0 -1.0 -1.0 -1.0 -1.0 -1.000000   \n",
       "\n",
       "     ...   4086  4087  4088  4089  4090  4091  4092  4093  4094  4095  \n",
       "58   ...      0     0     0     0     0     0     0     0     0     0  \n",
       "97   ...      0     0     0     0     0     0     0     0     0     0  \n",
       "98   ...      0     0     0     0     0     0     0     0     0     0  \n",
       "126  ...      0     0     0     0     0     0     0     0     0     0  \n",
       "136  ...      0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 16391 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = pd.HDFStore(data_loc + 'sampled_data.h5')\n",
    "df_features = pd.DataFrame(store['df' ])\n",
    "store.close()\n",
    "print('rows: {:,}, columns: {:,}'.format(len(df_features), len(df_features.columns)))\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get column names associated with a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names(file_name):\n",
    "    df = pd.read_csv(data_loc+file_name, index_col=0, nrows=0) # We need only the column names\n",
    "    df_cols = df.columns.tolist()\n",
    "    \n",
    "    # Take intersction with the most up to date, full feature set to drop those that have\n",
    "    # already been eliminated, e.g. because of zero variance columns removed in centroid-sampling.ipynb.\n",
    "    cols = df_features.columns.intersection(df_cols)\n",
    "    del df\n",
    "    del df_cols\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up old files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete old correlation files\n",
    "def clean_up():\n",
    "    subprocess.run('rm {}correlations/*'.format(data_loc), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df_data):\n",
    "    # whiten variables for fast parallel computation later on\n",
    "    df_data = (df_data - df_data.mean(axis=1, keepdims=True)) / df_data.std(axis=1, keepdims=True)\n",
    "\n",
    "    # save in binary format\n",
    "    np.save(data_loc+'samples', df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data for subset of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_get_data(file_name, samples=None):\n",
    "    clean_up()\n",
    "    cols = get_col_names(file_name)\n",
    "    \n",
    "    if sample is None:\n",
    "        X =  df_features[cols].values.T\n",
    "    else:\n",
    "        X =  df_features[cols].sample(n=samples, random_state=23).values.T\n",
    "        \n",
    "    pre_process(X)\n",
    "    print('Data shape: {}.'.format(X.shape))\n",
    "    del X\n",
    "    \n",
    "    # load samples as memory-map\n",
    "    return (np.load(data_loc+'samples.npy', mmap_mode='r'), cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deepgraph parallel computation of Pearson's Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connector function to compute pairwise pearson correlations\n",
    "# index_s and index_t are equal length arrays of indices for pairwise correlation\n",
    "def corr(index_s, index_t):\n",
    "    features_s = X[index_s].T\n",
    "    features_t = X[index_t].T\n",
    "    \n",
    "    c = 1./(n_samples - 1)\n",
    "    cov_xy = c * np.dot(features_s.T, features_t)\n",
    "    var_x = c * np.sum(features_s**2, axis=0)\n",
    "    var_y = c * np.sum(features_t**2, axis=0)\n",
    "    corrcoef_xy = cov_xy / np.sqrt(var_x[:, None] * var_y[None,:])\n",
    "    corr = np.diag(corrcoef_xy)\n",
    "    \n",
    "    del cov_xy\n",
    "    del var_x\n",
    "    del var_y\n",
    "    del corrcoef_xy\n",
    "    \n",
    "    #if 1947 in index_t:\n",
    "    #    print('corrcoef_xy shape: {}, index_s shape: {}, index_t shape: {}'.format(corrcoef_xy.shape, index_s.shape, index_t.shape))\n",
    "    \n",
    "    return corr\n",
    "    \n",
    "\n",
    "# parallel computation\n",
    "def create_ei(i):\n",
    "    from_pos = pos_array[i]\n",
    "    to_pos = pos_array[i+1]\n",
    "\n",
    "    # initiate DeepGraph\n",
    "    g = dg.DeepGraph(v)\n",
    "\n",
    "    # create edges\n",
    "    g.create_edges(connectors=corr, step_size=step_size,\n",
    "                   from_pos=from_pos, to_pos=to_pos)\n",
    "\n",
    "    # store edge table\n",
    "    g.e.to_pickle((data_loc+'correlations/{}.pickle').format(str(i).zfill(3)))\n",
    "\n",
    "\n",
    "# computation\n",
    "def calculate_correlation():\n",
    "    os.makedirs(data_loc+'correlations/', exist_ok=True)\n",
    "    indices = np.arange(0, n_processes - 1)\n",
    "    p = Pool()\n",
    "    for _ in p.imap_unordered(create_ei, indices):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_results():\n",
    "    # store correlation values\n",
    "    files = os.listdir(data_loc+'correlations/')\n",
    "    files.sort()\n",
    "    store = pd.HDFStore(data_loc+'e.h5', mode='w')\n",
    "    for f in files:\n",
    "        #print((data_loc+'correlations/{}').format(f))\n",
    "        et = pd.read_pickle((data_loc+'correlations/{}').format(f))\n",
    "        store.append('e', et, format='t', data_columns=True, index=False)\n",
    "    store.close()\n",
    "    \n",
    "    # load correlation table\n",
    "    e = pd.read_hdf(data_loc+'e.h5')\n",
    "    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highly_correlated_col_names(df_corr, cols):\n",
    "    pairs = list(df_corr.index)\n",
    "    p1 = [s for (s, t) in pairs]\n",
    "    p2 = [t for (s, t) in pairs]\n",
    "    \n",
    "    c = set(p1 + p2)\n",
    "    print('There are {:,} highly correlated columns for this set.'.format(len(c)))\n",
    "\n",
    "    col_names = [cols[i] for i in c]\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_to_drop(df_corr, cols):\n",
    "    pairs = list(df_corr.index)\n",
    "    p2 = set([t for (s, t) in pairs])\n",
    "\n",
    "    print('Dropping {:,} columns.'.format(len(p2)))\n",
    "\n",
    "    cols_to_drop = [cols[i] for i in p2]\n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of columns names to drop from first pass\n",
    "\n",
    "These are our candidates from the random 1000 row samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_first_pass = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAM control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 5e2\n",
    "n_processes = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drug_features/fingerprints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (4096, 1000).\n",
      "Number of highly correlated column pairs: 2\n",
      "There are 4 highly correlated columns for this set.\n"
     ]
    }
   ],
   "source": [
    "(X, cols) = get_get_data('drug_features/fingerprints.csv', samples=1000)\n",
    "\n",
    "n_features = len(X)\n",
    "n_samples = len(X[1])\n",
    "\n",
    "# index array for parallelization\n",
    "pos_array = np.array(np.linspace(0, n_features*(n_features-1)//2, n_processes), dtype=int)\n",
    "\n",
    "# create node table that stores references to the mem-mapped samples\n",
    "v = pd.DataFrame({'index': range(X.shape[0])})\n",
    "\n",
    "calculate_correlation()\n",
    "del v\n",
    "del X\n",
    "\n",
    "res = retrieve_results().abs()\n",
    "finger_print_corr = res[res['corr'] > 0.9]\n",
    "print('Number of highly correlated column pairs: {:,}'.format(len(finger_print_corr)))\n",
    "\n",
    "drop_cols_first_pass = drop_cols_first_pass + get_highly_correlated_col_names(finger_print_corr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drug_features/dragon_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2972, 1000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of highly correlated column pairs: 57,306\n",
      "There are 1,876 highly correlated columns for this set.\n"
     ]
    }
   ],
   "source": [
    "(X, cols) = get_get_data('drug_features/dragon_features.csv', samples=1000)\n",
    "\n",
    "n_features = len(X)\n",
    "n_samples = len(X[1])\n",
    "\n",
    "# index array for parallelization\n",
    "pos_array = np.array(np.linspace(0, n_features*(n_features-1)//2, n_processes), dtype=int)\n",
    "\n",
    "# create node table that stores references to the mem-mapped samples\n",
    "v = pd.DataFrame({'index': range(X.shape[0])})\n",
    "\n",
    "calculate_correlation()\n",
    "del v\n",
    "del X\n",
    "\n",
    "res = retrieve_results().abs()\n",
    "dragon_features_corr = res[res['corr'] > 0.9]\n",
    "print('Number of highly correlated column pairs: {:,}'.format(len(dragon_features_corr)))\n",
    "\n",
    "drop_cols_first_pass = drop_cols_first_pass + get_highly_correlated_col_names(dragon_features_corr, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.hist(bins=1000, figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### protein_features/binding_sites_v1.0.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (8417, 1000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of highly correlated column pairs: 3,301\n",
      "There are 1,132 highly correlated columns for this set.\n"
     ]
    }
   ],
   "source": [
    "(X, cols) = get_get_data('protein_features/binding_sites_v1.0.csv', samples=1000)\n",
    "\n",
    "n_features = len(X)\n",
    "n_samples = len(X[1])\n",
    "\n",
    "# index array for parallelization\n",
    "pos_array = np.array(np.linspace(0, n_features*(n_features-1)//2, n_processes), dtype=int)\n",
    "\n",
    "# create node table that stores references to the mem-mapped samples\n",
    "v = pd.DataFrame({'index': range(X.shape[0])})\n",
    "\n",
    "calculate_correlation()\n",
    "del v\n",
    "del X\n",
    "\n",
    "res = retrieve_results().abs()\n",
    "binding_sites_corr = res[res['corr'] > 0.9]\n",
    "print('Number of highly correlated column pairs: {:,}'.format(len(binding_sites_corr)))\n",
    "\n",
    "drop_cols_first_pass = drop_cols_first_pass + get_highly_correlated_col_names(binding_sites_corr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### protein_features/expasy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can skip this given that there're hardly any columns in this data set\n",
    "\n",
    "'''(X, cols) = get_get_data('protein_features/expasy.csv', samples=1000)\n",
    "\n",
    "n_features = len(X)\n",
    "n_samples = len(X[1])\n",
    "\n",
    "# index array for parallelization\n",
    "pos_array = np.array(np.linspace(0, n_features*(n_features-1)//2, n_processes), dtype=int)\n",
    "\n",
    "# create node table that stores references to the mem-mapped samples\n",
    "v = pd.DataFrame({'index': range(X.shape[0])})\n",
    "\n",
    "calculate_correlation()\n",
    "del v\n",
    "del X\n",
    "\n",
    "res = retrieve_results().abs()\n",
    "expasy_corr = res[res['corr'] > 0.9]\n",
    "print('Number of highly correlated column pairs: {:,}'.format(len(expasy_corr)))\n",
    "\n",
    "drop_cols = drop_cols + get_highly_correlated_col_names(expasy_corr, cols)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### protein_features/profeat.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (849, 1000).\n",
      "Number of highly correlated column pairs: 1,858\n",
      "There are 207 highly correlated columns for this set.\n"
     ]
    }
   ],
   "source": [
    "(X, cols) = get_get_data('protein_features/profeat.csv', samples=1000)\n",
    "\n",
    "n_features = len(X)\n",
    "n_samples = len(X[1])\n",
    "\n",
    "# index array for parallelization\n",
    "pos_array = np.array(np.linspace(0, n_features*(n_features-1)//2, n_processes), dtype=int)\n",
    "\n",
    "# create node table that stores references to the mem-mapped samples\n",
    "v = pd.DataFrame({'index': range(X.shape[0])})\n",
    "\n",
    "calculate_correlation()\n",
    "del v\n",
    "del X\n",
    "\n",
    "res = retrieve_results().abs()\n",
    "profeat_corr = res[res['corr'] > 0.9]\n",
    "print('Number of highly correlated column pairs: {:,}'.format(len(profeat_corr)))\n",
    "\n",
    "drop_cols_first_pass = drop_cols_first_pass + get_highly_correlated_col_names(profeat_corr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### protein_features/porter.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can skip this given that there're hardly any columns in this data set\n",
    "\n",
    "'''(X, cols) = get_get_data('protein_features/porter.csv', samples=1000)\n",
    "\n",
    "n_features = len(X)\n",
    "n_samples = len(X[1])\n",
    "\n",
    "# index array for parallelization\n",
    "pos_array = np.array(np.linspace(0, n_features*(n_features-1)//2, n_processes), dtype=int)\n",
    "\n",
    "# create node table that stores references to the mem-mapped samples\n",
    "v = pd.DataFrame({'index': range(X.shape[0])})\n",
    "\n",
    "calculate_correlation()\n",
    "del v\n",
    "del X\n",
    "\n",
    "res = retrieve_results().abs()\n",
    "porter_corr = res[res['corr'] > 0.9]\n",
    "print('Number of highly correlated column pairs: {:,}'.format(len(porter_corr)))\n",
    "\n",
    "drop_cols = drop_cols + get_highly_correlated_col_names(porter_corr, cols)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd pass\n",
    "\n",
    "Given all of the highly correlated columns in across the random n rows, re-test that set across all rows. The smaller set of columns should make it quite fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3,219 highly correlated columns across the data set.\n",
      "Data shape: (3219, 22172).\n",
      "Number of highly correlated column pairs: 63,607\n",
      "Dropping 1,661 columns.\n",
      "number of columns to drop: 1,661\n"
     ]
    }
   ],
   "source": [
    "print('There are {:,} highly correlated columns across the data set.'.format(len(drop_cols_first_pass)))\n",
    "\n",
    "clean_up()\n",
    "X =  df_features[drop_cols_first_pass].values.T\n",
    "    \n",
    "pre_process(X)\n",
    "print('Data shape: {}.'.format(X.shape))\n",
    "del X\n",
    "\n",
    "# load samples as memory-map\n",
    "X = np.load(data_loc+'samples.npy', mmap_mode='r')\n",
    "\n",
    "n_features = len(X)\n",
    "n_samples = len(X[1])\n",
    "\n",
    "# index array for parallelization\n",
    "pos_array = np.array(np.linspace(0, n_features*(n_features-1)//2, n_processes), dtype=int)\n",
    "\n",
    "# create node table that stores references to the mem-mapped samples\n",
    "v = pd.DataFrame({'index': range(X.shape[0])})\n",
    "\n",
    "calculate_correlation()\n",
    "del v\n",
    "del X\n",
    "\n",
    "res = retrieve_results().abs()\n",
    "df_corr = res[res['corr'] > 0.9]\n",
    "print('Number of highly correlated column pairs: {:,}'.format(len(df_corr)))\n",
    "\n",
    "cols_to_drop = get_cols_to_drop(df_corr, drop_cols_first_pass)\n",
    "\n",
    "print('number of columns to drop: {:,}'.format(len(cols_to_drop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the new data set without the correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_features.drop(cols_to_drop, axis=1)\n",
    "store = pd.HDFStore(data_loc + 'sampled_data.h5')\n",
    "store['df'] = df\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
